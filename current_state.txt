Our best results as of yet are as follows:
	- about 60% error on TEST and TRAIN, 70% on VAL

What we know:
	1) We are no longer including antecedents that are include or follow the trigger.
	2) It sometimes seems like, after getting to the best results above, the weight vectors
	   keep flip flopping, i.e. the error compensation is too high so it can't get to the
	   local minimum.

What we are doing:
	Loss function - between gold A* and some ant A:
		Return 1.0 - F1-score between the two, 
		i.e. the similarity btwn each word-pos tuple in each,
		ex: A*=[(NNP,dog), (VBP,ran)], A=[(NNP,dog), (VBZ,runs)]
		    - here only the (NNP,dog)s match, not the latter ones.
		
		UPDATE - we should ONLY consider THE EXACT WORD-TOKEN MATCH, i.e. the ACTUAL INSTANCE
			 the word number in the sentence, not tuples!!!!!!
	
	Alignment algorithm:
	Takes - T, A, D_want, where D_want is the dependencies we care about.
		0. T is the trigger's nearest encapsulating clause, so is A.
		1. Chunk T and A by the dependencies, d, in their sentdict iff d is in D_want.	
		   Chunking means creating a bunch of smaller nodes that encapsulate each sub-sentence
			contained in some dependency d.
		2. Remove from T and A the trigger & antecedent that are contained within them.
		3. Do the mapping:
			<><><><><><><><>
			UPDATE - make this a bipartite-graph matching algorithm 
			       - possibly we will change this because it could miss better mappings b/c its greedy
			       - show specific instances of alignments!! With correct and incorrect!!
			         and compare their feature vectors for human readability to compare distinctness

			For each chunk t in T:
				For each chunk a in A:
					if score(t,a), s > best_score:
						best_score = s
						best_achunk = a
				if best_score > threshold:
					remove t from T
					remove best_achunk from A
					mapping.append(t, best_achunk, best_score)
		   	<><><><><><><><>
		    Based on scoring function:
			score(t,a): 
				score = 0.0
				if t.depname == a.depname: 
					score += 3
				score += f1_similarity(t.words, a.words) * words_weight
				score += f1_similarity(t.pos, a.pos) * pos_weight
				score += f1_similarity(t.lemmas, a.lemmas) * lemmas_weight
				
				score -= abs(i-j)/len(T_original) # Distance btwn chunk positions
				UPDATE SUBTRACTION - 				
	
		4. Vectorize the mapping:
			Using one-hot, encode:
				- un_mapped trigger and antecedent chunks (separately)
				- length of the mapping # might be a bad feature
				- the mapped deps for trigger and antecedent that we are interested in
			Also encode the mean, st. deviation, min, and max of the scores.
			UPDATE:
				- negative features! If special deps are in one but not the other
			JACKIE: the correct and incorrect alignments will probably not be too distinguishable

Parameters:
	Learning rate
	C (for the optimization program's weight update)
	K (for the number of potential antecedents MIRA considers)
	Weights initialization
	The arbitrary values for the alignment scoring (probably not necessary to do)

Questions:
	1) Why aren't we overfitting? Why do we sometimes do better on TEST as TRAIN?
	2) Is the alignment algorithm working well?
	3) How do good alignments compare with bad alignments?
	4) Is the loss function good enough?
	5) Is the alignment scoring function good enough? How can we change its arbitraryness?


